\documentclass{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{multicol}
\usepackage{paralist}
\usepackage{todonotes}
\setlength{\marginparwidth}{2.15cm}
\usepackage{booktabs}
\usepackage{enumitem}
\graphicspath{{../}}
\usepackage{setspace}
\doublespacing

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\section*{}
\begin{center}
  \centerline{\textsc{\LARGE Homework 3}}
  \vspace{0.5em}
  \centerline{\textsc{Regression, Gaussian Processes, and Boosting}}
  \vspace{1em}
  \textsc{\large Dana Van Aken} \\
\end{center}

\section*{Problem 1: Gaussian Processes}

\begin{enumerate}[label=(\alph*)]
\setlength\itemsep{1em}

\item % (a)

\item % (b)

\item % (c)

\item % (d)

\item % (e)

\item % (f)

\end{enumerate}

\section*{Problem 2: Regression}

\subsection*{2.1 Why Lasso Works}

\begin{enumerate}
\setlength\itemsep{1em}

\item Write $J_\lambda(\beta)$ in the form $J_\lambda(\beta)=g(y)+\sum_1^df(X_i,y,\beta_i,\lambda)$, $\lambda > 0$: \\
$J_\lambda(\beta)=\frac{1}{2}\norm{y-X\beta}^2+\lambda\norm{\beta}$ \\
$=\frac{1}{2}(y-X\beta)^T(y-X\beta)+\lambda\norm{\beta}$ \\
$=\frac{1}{2}[\norm{y}^2-2y^TX\beta+(X\beta)^TX\beta]+\lambda\norm{\beta}$ \\
$=\frac{1}{2}[\norm{y}^2-2y^TX\beta+\beta^TX^TX\beta]+\lambda\norm{\beta}$ \\
$=\frac{1}{2}[\norm{y}^2-2y^TX\beta+\beta^T\beta]+\lambda\norm{\beta}$ \hfill $(X^TX=I)$ \\
$=\frac{1}{2}\norm{y}^2-y^TX\beta+\frac{1}{2}\norm{\beta}^2+\lambda\norm{\beta}$

\item % (2)

\item % (3)

\item % (4)

\item % (5)

\end{enumerate}
\subsection*{2.2 Bayesian regression and Gaussian process}

\begin{enumerate}
\setlength\itemsep{1em}

\item % (1)

\begin{enumerate}

\item % (a)

\item % (b)

\end{enumerate}

\item % (2)

\item % (3)

\item % (4)

\end{enumerate}
\end{document}